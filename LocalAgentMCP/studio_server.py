import uvicorn
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import json
import os
import asyncio
import httpx
from tool_loader import load_all_tools

# ================= CONFIG =================
PORT = 8000
OLLAMA_URL = "http://localhost:11434/api/chat"
CONFIG_FILE = "ai_config.json"

app = FastAPI(title="Local AI Studio")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ë¡œì»¬ íˆ´ ë¡œë“œ (ìš°ë¦¬ê°€ ë§Œë“  íŒŒì¼/ìœ ë‹ˆí‹° ì œì–´ íˆ´)
TOOLS = load_all_tools()

# ëª¨ë¸ ì„¤ì • ë¡œë“œ
def get_config():
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

# ================= TOOL EXECUTOR =================
async def execute_tool(tool_name, args):
    if tool_name not in TOOLS:
        return f"Error: Tool '{tool_name}' not found."
    
    try:
        handler = TOOLS[tool_name]["handler"]
        print(f"ğŸ”§ [Tool Run] {tool_name} with {args}")
        if asyncio.iscoroutinefunction(handler):
            result = await handler(args)
        else:
            result = await asyncio.to_thread(handler, args)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        return f"Error executing {tool_name}: {str(e)}"

# ================= API & WEBSOCKET =================
@app.get("/")
async def get_ui():
    with open("studio.html", "r", encoding="utf-8") as f:
        return HTMLResponse(f.read())

@app.get("/models")
async def list_models():
    return get_config().get("models", {})

@app.websocket("/ws/chat")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    history = [] # ëŒ€í™” ë¬¸ë§¥ ìœ ì§€
    
    try:
        while True:
            data = await websocket.receive_json()
            user_msg = data.get("message")
            model_key = data.get("model", "fast_coding")
            
            config = get_config()
            model_name = config["models"][model_key]["name"]
            sys_prompt = config.get("system_prompt", "")

            # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            history.append({"role": "user", "content": user_msg})
            
            # 2. ë„êµ¬ ì •ì˜ (Ollama í¬ë§·)
            ollama_tools = []
            for name, info in TOOLS.items():
                ollama_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": info.get("description", ""),
                        "parameters": info.get("inputSchema", {})
                    }
                })

            # 3. Ollama í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì•„ë‹˜ - íˆ´ ì‚¬ìš© íŒë‹¨ì„ ìœ„í•´)
            payload = {
                "model": model_name,
                "messages": [{"role": "system", "content": sys_prompt}] + history,
                "tools": ollama_tools,
                "stream": False
            }

            # UIì— "ìƒê° ì¤‘..." í‘œì‹œ
            await websocket.send_json({"type": "status", "text": "ğŸ¤” AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."})

            async with httpx.AsyncClient(timeout=120) as client:
                resp = await client.post(OLLAMA_URL, json=payload)
                resp_json = resp.json()
                ai_msg = resp_json.get("message", {})

                # 4. íˆ´ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                if ai_msg.get("tool_calls"):
                    # AIê°€ ë„êµ¬ë¥¼ ì“°ê² ë‹¤ê³  í•¨
                    history.append(ai_msg) # AIì˜ ì˜ë„ë¥¼ ê¸°ë¡
                    
                    for tool_call in ai_msg["tool_calls"]:
                        fn = tool_call["function"]
                        t_name = fn["name"]
                        t_args = fn["arguments"]
                        
                        await websocket.send_json({"type": "status", "text": f"ğŸ› ï¸ ë„êµ¬ ì‹¤í–‰ ì¤‘: {t_name}..."})
                        
                        # ë„êµ¬ ì‹¤í–‰!
                        tool_result = await execute_tool(t_name, t_args)
                        
                        # ê²°ê³¼ ê¸°ë¡
                        history.append({
                            "role": "tool",
                            "content": tool_result,
                        })
                    
                    # 5. ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ìš”ì²­
                    payload["messages"] = [{"role": "system", "content": sys_prompt}] + history
                    del payload["tools"] # ìµœì¢… ë‹µë³€ ë• íˆ´ ë” (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                    
                    final_resp = await client.post(OLLAMA_URL, json=payload)
                    final_msg = final_resp.json()["message"]["content"]
                    
                    history.append({"role": "assistant", "content": final_msg})
                    await websocket.send_json({"type": "answer", "text": final_msg})

                else:
                    # ë„êµ¬ ì•ˆ ì“°ê³  ë°”ë¡œ ëŒ€ë‹µí•¨
                    content = ai_msg.get("content", "")
                    history.append({"role": "assistant", "content": content})
                    await websocket.send_json({"type": "answer", "text": content})

    except WebSocketDisconnect:
        print("Client disconnected")

if __name__ == "__main__":
    # uvicorn studio_server:app --reload
    uvicorn.run(app, host="0.0.0.0", port=PORT)