import uvicorn
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import json
import os
import asyncio
import httpx
import glob
from tool_loader import load_all_tools

# ================= CONFIG =================
PORT = 8000
DEFAULT_OLLAMA_URL = "http://localhost:11434/api/chat"
CONFIG_FILE = "ai_config.json"

app = FastAPI(title="Local AI Studio")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], 
    allow_methods=["*"], 
    allow_headers=["*"]
)

# Static í´ë” ë§ˆìš´íŠ¸ (ì´ë¯¸ì§€ ì„œë¹™ìš©)
if not os.path.exists("static"):
    os.makedirs("static")
app.mount("/static", StaticFiles(directory="static"), name="static")

# ë¡œì»¬ íˆ´ ë¡œë“œ
TOOLS = load_all_tools()

# ëŒ€í™” ë‚´ì—­ ì €ì¥ì†Œ
history_storage = {}

# ================= HELPER FUNCTIONS =================
def parse_ovos_json(data):
    """OVOS ìŠ¤íƒ€ì¼ JSONì„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    prompt = f"ë‹¹ì‹ ì€ '{data.get('name')}'ì…ë‹ˆë‹¤. ì—­í• ì€ '{data.get('role')}'ì…ë‹ˆë‹¤.\n"
    prompt += f"ì„¤ëª…: {data.get('description')}\n\n"
    
    prompt += "â˜…â˜…â˜… ì¤‘ìš”: ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ë„êµ¬(web_search)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ìµœì‹  ì •ë³´ë¥¼ ë¬»ê±°ë‚˜ ë‹¹ì‹ ì´ ëª¨ë¥´ëŠ” ì§€ì‹ì„ ë¬¼ì–´ë³´ë©´, ì ˆëŒ€ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µí•˜ì§€ ë§ê³  ì¦‰ì‹œ 'web_search' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”. â˜…â˜…â˜…\n\n"
    
    if "speech_style" in data:
        style = data["speech_style"]
        prompt += f"[ë§íˆ¬ ê°€ì´ë“œ]\n- í†¤: {style.get('tone')}\n"
        prompt += f"- íŠ¹ì§•: {style.get('sentence_pattern')}\n"
        prompt += "- ì˜ˆì‹œ:\n"
        for ex in style.get("examples", []):
            prompt += f"  * {ex}\n"
    
    if "interaction_rules" in data:
        rules = data["interaction_rules"]
        prompt += "\n[í–‰ë™ ìˆ˜ì¹™]\n"
        for rule in rules.get("always", []):
            prompt += f"- (í•­ìƒ) {rule}\n"
        for rule in rules.get("never", []):
            prompt += f"- (ê¸ˆì§€) {rule}\n"
            
    return prompt

def load_character_plugins():
    """ìºë¦­í„° í”ŒëŸ¬ê·¸ì¸ ë¡œë“œ"""
    models = {}
    if not os.path.exists("characters"):
        os.makedirs("characters")
    
    for filepath in glob.glob("characters/*.json"):
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
                char_id = os.path.basename(filepath).replace(".json", "")
                
                sys_prompt = parse_ovos_json(data) if "speech_style" in data else data.get("system_prompt", "")
                
                models[char_id] = {
                    "name": data.get("base_model", "qwen2.5-coder:14b"),
                    "label": f"{data.get('name', char_id)}",
                    "role_badge": data.get('role', 'Assistant'),
                    "description": data.get('description', ''),
                    "icon": data.get("icon", "fa-user"),
                    "system_prompt": sys_prompt
                }
                if char_id not in history_storage:
                    history_storage[char_id] = []
        except Exception as e:
            print(f"âŒ Error loading {filepath}: {e}")
            
    return models

def get_config():
    config = {"models": {}}
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                config = json.load(f)
        except:
            pass
    
    config["models"].update(load_character_plugins())
    return config

async def execute_tool(tool_name, args):
    """íˆ´ ì‹¤í–‰"""
    if tool_name not in TOOLS:
        return f"Error: Tool '{tool_name}' not found."
    
    try:
        handler = TOOLS[tool_name]["handler"]
        print(f"ğŸ”§ [Tool Run] {tool_name}")
        
        if asyncio.iscoroutinefunction(handler):
            result = await handler(args)
        else:
            result = await asyncio.to_thread(handler, args)
            
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        return f"Error executing {tool_name}: {str(e)}"

async def analyze_image_with_vision_model(image_base64, prompt="ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì¤˜."):
    """Vision ëª¨ë¸(Llava) í˜¸ì¶œ"""
    try:
        payload = {
            "model": "llava",
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [image_base64]
                }
            ],
            "stream": False
        }
        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(DEFAULT_OLLAMA_URL, json=payload)
            if resp.status_code == 200:
                return resp.json()["message"]["content"]
    except:
        return None
    return None

# ================= API ENDPOINTS =================
@app.get("/")
async def get_ui():
    if os.path.exists("studio.html"):
        with open("studio.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    return HTMLResponse("<h1>studio.html not found</h1>")

@app.get("/models")
async def list_models():
    return get_config().get("models", {})

@app.get("/history/{model_id}")
async def get_history(model_id: str):
    return history_storage.get(model_id, [])

# ================= WEBSOCKET CORE =================
@app.websocket("/ws/chat")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    try:
        while True:
            try:
                # 1. ë°ì´í„° ìˆ˜ì‹  ë° íƒ€ì… ë°©ì–´
                data = await websocket.receive_json()
                
                if isinstance(data, str):
                    try:
                        data = json.loads(data)
                    except:
                        continue
                
                if not isinstance(data, dict):
                    continue

            except WebSocketDisconnect:
                break
            except Exception:
                break
            
            user_msg = data.get("message", "")
            files = data.get("files", [])
            model_key = data.get("model", "lucia")
            
            config = get_config()
            if model_key not in config["models"]:
                model_key = list(config["models"].keys())[0] if config["models"] else None
            
            current_model = config["models"].get(model_key)
            if not current_model:
                continue

            if model_key not in history_storage:
                history_storage[model_key] = []

            # 2. ì´ë¯¸ì§€ ì²˜ë¦¬ (Vision Proxy)
            images_processed_context = ""
            for f in files:
                if f['type'] == 'image':
                    await websocket.send_json({"type": "status", "text": "ğŸ‘ï¸ ì´ë¯¸ì§€ë¥¼ ë³´ëŠ” ì¤‘..."})
                    vision_result = await analyze_image_with_vision_model(f['content'])
                    if vision_result:
                        images_processed_context += f"\n[ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: {vision_result}]\n"
                    else:
                        images_processed_context += "\n[ì‹œìŠ¤í…œ: ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ (Llava ëª¨ë¸ í•„ìš”)]\n"
                elif f['type'] == 'text':
                    images_processed_context += f"\n\n--- [íŒŒì¼: {f['name']}] ---\n{f['content']}\n------------------\n"

            final_user_msg = images_processed_context + "\n" + user_msg if images_processed_context else user_msg
            history_storage[model_key].append({"role": "user", "content": final_user_msg})

            # 3. Ollama í˜¸ì¶œ ì¤€ë¹„
            ollama_tools = []
            for name, info in TOOLS.items():
                ollama_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": info.get("description", ""),
                        "parameters": info.get("inputSchema", {})

            try:
                async with httpx.AsyncClient(timeout=180) as client:
                    resp = await client.post(DEFAULT_OLLAMA_URL, json=payload)
                    resp.raise_for_status()
                    
                    # [í•µì‹¬] ì‘ë‹µ ë°ì´í„° íŒŒì‹± ë° íƒ€ì… ë°©ì–´
                    try:
                        resp_data = resp.json()
                    except:
                        resp_data = {}

                    # ë¬¸ìì—´ë¡œ ì˜¤ë©´ JSON íŒŒì‹± ì¬ì‹œë„
                    if isinstance(resp_data, str):
                        try:
                            resp_data = json.loads(resp_data)
                        except:
                            resp_data = {"message": {"content": str(resp_data)}}

                    ai_msg = resp_data.get("message", {})

                    # [â˜…ì—¬ê¸°ê°€ ìˆ˜ì •ë¨â˜…] message í•„ë“œê°€ ë¬¸ìì—´ì´ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    if isinstance(ai_msg, str):
                        ai_msg = {"content": ai_msg}

                    # 4. íˆ´ ì‚¬ìš© ì—¬ë¶€ ì²´í¬
                    if isinstance(ai_msg, dict) and ai_msg.get("tool_calls"):
                        history_storage[model_key].append(ai_msg)
                        
                        for tool_call in ai_msg["tool_calls"]:
                            fn = tool_call["function"]
                            t_name = fn["name"]
                            t_args = fn["arguments"]
                            
                            await websocket.send_json({"type": "status", "text": f"ğŸ’» {t_name} ì‹¤í–‰ ì¤‘..."})
                            
                            # íˆ´ ì‹¤í–‰
                            tool_result = await execute_tool(t_name, t_args)
                            
                            history_storage[model_key].append({
                                "role": "tool",
                                "content": tool_result,
                            })
                        
                        # íˆ´ ê²°ê³¼ ë°˜ì˜ í›„ ì¬í˜¸ì¶œ
                        payload["messages"] = [{"role": "system", "content": current_model.get("system_prompt", "")}] + history_storage[model_key]
                        del payload["tools"]
                        
                        final_resp = await client.post(DEFAULT_OLLAMA_URL, json=payload)
                        final_data = final_resp.json()
                        
                        final_msg_obj = final_data.get("message", {})
                        if isinstance(final_msg_obj, str):
                            final_msg_obj = {"content": final_msg_obj}
                            
                        final_content = final_msg_obj.get("content", "")
                        
                        history_storage[model_key].append({"role": "assistant", "content": final_content})
                        await websocket.send_json({"type": "answer", "text": final_content})

                    else:
                        # ì¼ë°˜ ë‹µë³€
                        content = ai_msg.get("content", "")
                        history_storage[model_key].append({"role": "assistant", "content": content})
                        await websocket.send_json({"type": "answer", "text": content})

            except Exception as e:
                print(f"Error: {e}")
                await websocket.send_json({"type": "error", "text": f"AI ì‘ë‹µ ì‹¤íŒ¨: {str(e)}"})

    except Exception as e:
        print(f"WebSocket Error: {e}")
    finally:
        print("Client disconnected.")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)